{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIunq8mQTdmn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kMU2cA5QfyM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# input Embedding class: Here in nn module we have inbuilt embedding function\n",
        "#  we have used to embed based on vocabulary size and length of the vector\n",
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, vocab_size: int, d_model: int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding= nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)\n"
      ],
      "metadata": {
        "id": "XRBGhBtUT81j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding class\n",
        "# As we know the positional encoding will not change through traininig\n",
        "# We have fixed formulas for that for even position we have sin and for odd we have cos function formulas\n",
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model: int, dropout: float =0.1,max_len: int=5000):\n",
        "    super().__init()\n",
        "    self.d_model=d_model\n",
        "    self.max_len=max_len\n",
        "    self.dropout=nn.dropout(dropout)\n",
        "    pe=torch.zeros(max_len, d_model)\n",
        "    position=torch.arange(0,max_len, dtype=torch.float).unsqueeze(1)\n",
        "    dim_term=torch.exp(torch.arange(0,d_model,2).float() *(-math.log(10000.0)/ d_model))\n",
        "    pe[:,0::2]= torch.sin(position* dim_term)\n",
        "    pe[:,1::2]= torch.cos(position* dim_term)\n",
        "\n",
        "    pe= pe.unsqueeze(0)\n",
        "\n",
        "    self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self,x):\n",
        "      x= x+ (self.pe[:,:x.shape[1],:]).requires_grad_(False)\n",
        "      return self.dropout(x)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U7nhDMPUduNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Layer Normalization class\n",
        "class Layer_Normalization(nn.Module):\n",
        "  def __init__(self, eps: float = 10**-6):\n",
        "    super().__init__()\n",
        "    self.eps=eps\n",
        "    self.alpha= nn.parameter(torch.ones(1))\n",
        "    self.bias= nn.parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean= x.mean(dim=-1,keepdim=True)\n",
        "    std= x.std(dim=-1,keepdim=True)\n",
        "    return self.alpha *(x-mean) /(std-self.eps) + self.bias\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ol3TdVqblOqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feed Forward Layer\n",
        "class FeedForwardLayer(nn.Module):\n",
        "  def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "    super().__init__()\n",
        "    self.Linear_1= nn.Linear(d_model, d_ff)\n",
        "    self.Dropout= nn.Dropout(dropout)\n",
        "    self.Linear_2= nn.Linear(d_ff, d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "     x= self.Linear_1(x)\n",
        "     x= F.relu(x)\n",
        "     x= self.Dropout(x)\n",
        "     x= self.Linear_2(x)\n",
        "     return x\n"
      ],
      "metadata": {
        "id": "_L8BZ9pLrUuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Multi Head Attention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model: int, h : int, dropout: float):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.h=h\n",
        "    assert d_model % h ==0,\"d_model is not divisible by h\"\n",
        "    self.d_k =d_model // h\n",
        "\n",
        "    self.w_q=nn.Linear(d_model,d_model)\n",
        "    self.w_v=nn.Linear(d_model,d_model)\n",
        "    self.w_k=nn.Linear(d_model,d_model)\n",
        "\n",
        "    self.w_o = nn.Linear(d_model, d_model)\n",
        "    self.dropout ==nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query,key,value,mask, dropout: nn.Dropout):\n",
        "    d_k= query.shape[-1]\n",
        "\n",
        "    attention_scores =(query @ key.transpose(-2,-1))/math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask==0, -1e9)\n",
        "    attention_scores= attention_scores.softmax(dim=-1)\n",
        "    if dropout is not None:\n",
        "      attention_scores= dropout(attention_scores)\n",
        "\n",
        "    return (attention_scores @ value), attention_scores\n",
        "\n",
        "  def forward(self, q,k,v, mask):\n",
        "    query=self.w_q(q)\n",
        "    key=self.w_k(k)\n",
        "    value=self.w_v(v)\n",
        "\n",
        "    query= query.view(query.shape[0],query.shape[1],self.h, self.d_k).transpose(1,2)\n",
        "    key= key.view(key.shape[0],key.shape[1],self.h, self.d_k).transpose(1,2)\n",
        "    value=value.view(value.shape[0],value.shape[1],self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "    x,self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "\n",
        "    x= x.transpose(1,2).contiguous()\n",
        "    x= x.view(x.shape[0],-1,self.h*self.d_k)\n",
        "    x= self.w_o(x)\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5VD0T-bZthzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual connection\n",
        "class ResidualConnection(nn.Module):\n",
        "  def __init__(self,dropout: float):\n",
        "    super().__init()\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.norm=Layer_Normalization()\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "    return x+ self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "wtVVzTAW5cdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EncoderLayer\n",
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,self_attention_block: MultiHeadAttention, feed_forward_layer:FeedForwardLayer, dropout:float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block=self_attention_block\n",
        "    self.feed_forward_layer=feed_forward_layer\n",
        "    self.residual_connection=nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x= self.residual_connection[0](x, lambda x: self.self_attention_block(x,x,x,mask))\n",
        "    x= self.residual_connection[1](x, self.feed_forward_layer)\n",
        "    return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers=layers\n",
        "    self.norm=Layer_Normalization()\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    for layer in self.layers:\n",
        "      x= layer(x,mask)\n",
        "    return self.norm(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "DPmGMUAVcg76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        " def __init__(self,self_attention_block: MultiHeadAttention, feed_forward_layer:FeedForwardLayer, dropout:float):\n",
        "    super().__init__()\n",
        "    self.self_attention_block=self_attention_block\n",
        "    self.feed_forward_layer=feed_forward_layer\n",
        "    self.residual_connection=nn.ModuleList([ResidualConnection(dropout) for _ in range(3)])\n",
        " def forward(self, x,encoder_output, src_mask,trt_mask):\n",
        "    x= self.residual_connection[0](x, lambda x: self.self_attention_block(x,x,x,trt_mask))\n",
        "    x= self.residual_connection[1](x, lambda x: self.self_attention_block(x,encoder_output,encoder_output,src_mask))\n",
        "    x= self.residual_connection[2](x, self.feed_forward_layer)\n",
        "    return x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, layers: nn.ModuleList):\n",
        "    super().__init__()\n",
        "    self.layers=layers\n",
        "    self.norm=Layer_Normalization()\n",
        "\n",
        "  def forward(self, x,encoder_output, src_mask,trt_mask):\n",
        "    for layer in self.layers:\n",
        "      x= layer(x,encoder_output, src_mask,trt_mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "wWz9s3wGiB7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ProjectionLayer\n",
        "class projectionLayer(nn.Module):\n",
        "  def __init__(self,d_model: int, vocab_size: int):\n",
        "    super().__Init__()\n",
        "    self.linear=nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return F.log_softmax(self.linear(x),dim=-1)"
      ],
      "metadata": {
        "id": "jT_ZFlKMmltZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Transformer\n",
        "\n",
        "class transformer(nn.Module):\n",
        "  def __init__(self,encoder: Encoder,decoder: Decoder, projection_layer: projectionLayer,src_emb: InputEmbedding, trt_emb: InputEmbedding,src_pos: PositionalEncoding, trt_emb: PositionalEncoding):\n",
        "    super().__init__()\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.projection_layer=projection_layer\n",
        "    self.src_emb=src_emb\n",
        "    self.trt_emb=trt_emb\n",
        "    self.src_pos=src_pos\n",
        "    self.trt_pos=trt_pos\n",
        "  def encoder_forward(self,src,src_mask):\n",
        "    src= self.src_emb(src)\n",
        "    src= self.src_pos(src)\n",
        "    return self.encoder(src,src_mask)\n",
        "  def trt_forward(self,trt,trt_mask):\n",
        "    trt= self.trt_emb(trt)\n",
        "    trt= self.trt_pos(trt)\n",
        "    return self.decoder(trt,encoder_output,src_mask,trt_mask)\n",
        "  def project(self,x):\n",
        "    return self.projection_layer(x)\n",
        "  def build_Transformer(src_vocab_size: int,trt_vocab_size: int, d_model: int= 512,src_seq_len: int,tgt_seq_len: int, dropout: float=0.01, d_ff: int=2048, N:int= 6,h:int =8)-> Transformer:\n",
        "    src_embed= InputEmbedding(src_vocab_size,d_model)\n",
        "    trt_embed= InputEmbedding(trt_vocab_size,d_model)\n",
        "    src_pos= PositionalEncoding(d_mode,src_seq_len,dropout)\n",
        "    trt_pos= PositionalEncoding(d_model,tgt_seq_len,dropout)\n",
        "\n",
        "    encoder_blocks=[]\n",
        "    for _ in range(N):\n",
        "      encoder_self_attention_block= MultiHeadAttention(d_model,h,dropout)\n",
        "      feed_forward_block= FeedForwardLayer(d_model,d_ff,dropout)\n",
        "      encoder_block= EncoderBlock(encoder_self_attention_block,feed_forward_block,dropout)\n",
        "      encoder_blocks.append(encoder_block)\n",
        "    decoder_blocks=[]\n",
        "    for _ in range(N):\n",
        "      encoder_self_attention_block= MultiHeadAttention(d_model,h,dropout)\n",
        "      decoder_cross_attention_block= MultiHeadAttention(d_model,h,dropout)\n",
        "      feed_forward_block= FeedForwardLayer(d_model,d_ff,dropout)\n",
        "      decoder_block= DecoderBlock(decoder_cross_attention_block,feed_forward_block,dropout)\n",
        "\n",
        "      decoder_blocks.append(decoder_block)\n",
        "\n",
        "      ecoder= Encoder(nn.ModuleList(encoder_blocks))\n",
        "      decoder= Decoder(nn.ModuleList(decoder_blocks))\n",
        "\n",
        "      projection_layer= projectionLayer(d_model,trt_vocab_size)\n",
        "      transformer= Transformer(encoder,decoder,projection_layer,src_embed,trt_embed,src_pos,trt_pos)\n",
        "      #initialize parameters\n",
        "      for p in transformer.parameters():\n",
        "      if p.dim()>1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "      return transformer\n"
      ],
      "metadata": {
        "id": "iy1TLgWzyW6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OMAj_aiz5T-k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}